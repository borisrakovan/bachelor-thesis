"""clutrr_neuralogic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14uxJfEy4zJBFhLG6kYvkIEqDsiC-VUZ0
"""



#%%
from notebooks.clutrr import clutrr_config, Instance, Data as ClutrrData

clutrr_data = ClutrrData(clutrr_config.train_path, clutrr_config.test_paths)

print(clutrr_data.train[0].target)

#%%


# entity_lst, relation_lst = data.entity_lst, data.relation_lst

# relation_to_predicate = data.relation_to_predicate

# relation_to_idx = {r: i for i, r in enumerate(relation_lst)}

# nb_nodes = len(entity_lst)
# nb_edge_types = len(relation_lst)

print(clutrr_config.train_path)
print(f"len(data.train)={len(clutrr_data.train)}")

from neuralogic.core import Relation, Dataset, Data

import itertools
from collections import defaultdict

from neuralogic.core.constructs.atom import WeightedAtom
from typing import List
import random

random.seed(21)

relation_lst = ['uncle', 'mother-in-law', 'father', 'daughter-in-law', 'granddaughter', 'grandson', 'sister', 'niece', 'aunt', 'father-in-law', 'nephew', 'husband', 'brother', 'mother', 'daughter', 'son-in-law', 'son', 'wife', 'grandfather', 'grandmother']
relation_to_idx = {r: i for i, r in enumerate(relation_lst)}

print(f"num relations = {len(relation_lst)}")


class DatasetFactory:
    def __init__(self):
        self.all_persons = set()
        self.persons_counts = defaultdict(int)

    def person_id(self, p: str):
        """Persons across different graphs must be unique even though their names are the same"""
        return f"{p}_{self.persons_counts[p]}"

    def increment_counts(self):
        for p in self.persons_counts:
            self.persons_counts[p] += 1

    def make_example(self, instance: Instance):
        persons = set(p.lower() for p, _, _ in instance.story)
        persons.update(p.lower() for _, _, p in instance.story)

        relations = set(r.lower() for _, r, _ in instance.story + [instance.target])
        persons_map = {p: self.person_id(p) for p in persons}

        story = []
        for t0, t1, t2 in instance.story:
            t0, t1, t2 = t0.lower(), t1.lower(), t2.lower()
            story.append([persons_map[t0], t1, persons_map[t2]])

        self.all_persons.update(persons_map.values())

        example = list(itertools.chain(
            (Relation.edge(u, r, v) for u, r, v in story),
            (Relation.rel(r) for r in relations),
            (Relation.person(persons_map[p]) for p in persons)
        ))

        return example

    def make_query(self, target: List[str]) -> WeightedAtom:
        ent1, rel, ent2 = target

        one_hot_relation = [0] * len(relation_lst)
        one_hot_relation[relation_to_idx[rel]] = 1
        return Relation.predict(self.person_id(ent1), self.person_id(ent2))[one_hot_relation]

    def to_example_query(self, instance: Instance):
        example = self.make_example(instance)
        ltarget = [x.lower() for x in instance.target]
        query = self.make_query(ltarget)

        self.increment_counts()
        
        return example, query


dataset = Dataset()
fac = DatasetFactory()

train_labels = []

for i in range(len(clutrr_data.train)):
    instance = clutrr_data.train[i]
    train_labels.append(instance.target[1])
    example, query = fac.to_example_query(instance)

    dataset.add_example(example)
    dataset.add_query(query)

from collections import Counter
train_labels_stats = Counter(train_labels)
print(train_labels_stats.items())
print(len(train_labels_stats))

# test datasets sorted wrt difficulty
test_datasets, test_names = [], []

for test_name, instances in clutrr_data.test.items():
    test_dataset = Dataset()
    for inst in instances:
        example, query = fac.to_example_query(inst)
        test_dataset.add_example(example)
        test_dataset.add_query(query)
    test_datasets.append(test_dataset)
    test_names.append(test_name)

print(f"len(test_datasets)={len(test_datasets)}")

#%%

"""## Template"""

print("SAMPLE\n")
print("Examples:")
for ex in test_datasets[0].examples[0]:
    print(ex.to_str())
print("Query:")
print(test_datasets[0].queries[0])

from neuralogic.core import Template, R, V, Activation, Aggregation

#%% RGCN + 2-S RGCN
metadata = [Aggregation.AVG, Activation.IDENTITY]

template = Template()

w = 24

template += R.h_0(V.X)[w,] <= R.person(V.X)  # Project into vector

# identity( W0 * h0 foreach  )
template += (R.h_1(V.I) <= R.h_0(V.I)[w, w]) | metadata

for relation in relation_lst:
    template += (R.h_1(V.I) <= (R.h_0(V.J)[w, w], R.hidden.edge(V.I, relation, V.J))) | metadata

    # Uncomment to get a better accuracy - it doesn't really correspond to RCGN
    template += ( R.h_1(V.I) <= (R.h_0(V.J)[w, w], R.hidden.edge(V.J, relation, V.I)) ) | metadata

template += R.h_1 / 1 | [Activation.SIGMOID]

template += (R.h_2(V.I) <= R.h_1(V.I)[w, w]) | metadata

for relation in relation_lst:
    template += (R.h_2(V.I) <= (R.h_1(V.J)[w, w], R.hidden.edge(V.I, relation, V.J))) | metadata

    # davam volnost aby relace nemuseli byt symetricky - paremetrizuji 1 smer samost vahama a druhy samost vahama

    # Uncomment to get a better accuracy - it doesn't really correspond to RCGN
    template += (R.h_2(V.I) <= (R.h_1(V.J)[w, w], R.hidden.edge(V.J, relation, V.I))) | metadata

template += R.h_2 / 1 | [Activation.SIGMOID]

template += R.predict(V.X, V.Y)[20, w] <= (R.h_2(V.X)[w, w], R.h_2(V.Y)[w, w])
template += R.predict / 2 | [Activation.SIGMOID]

#%% RGCN with shared relation weights


def weight_name(rel: str, weight: str):
    return f'{weight}_{rel.replace("-", "_")}'

metadata = [Aggregation.AVG, Activation.IDENTITY]

template = Template()

w = 24

template += R.h_0(V.X)[w,] <= R.person(V.X)  # Project into vector

# identity( W0 * h0 foreach  )
template += (R.h_1(V.I) <= R.h_0(V.I)[w, w]) | metadata

for relation in relation_lst:
    W1 = weight_name(relation, "W1")
    W2 = weight_name(relation, "W2")
    template += (R.h_1(V.I) <= (R.h_0(V.J)[W1: w, w], R.hidden.edge(V.I, relation, V.J))) | metadata

    # Uncomment to get a better accuracy - it doesn't really correspond to RCGN
    template += (R.h_1(V.I) <= (R.h_0(V.J)[W2: w, w], R.hidden.edge(V.J, relation, V.I))) | metadata

template += R.h_1 / 1 | [Activation.SIGMOID]

template += (R.h_2(V.I) <= R.h_1(V.I)[w, w]) | metadata

for relation in relation_lst:
    W1 = weight_name(relation, "W1")
    W2 = weight_name(relation, "W2")
    template += (R.h_2(V.I) <= (R.h_1(V.J)[W1: w, w], R.hidden.edge(V.I, relation, V.J))) | metadata

    # davam volnost aby relace nemuseli byt symetricky - paremetrizuji 1 smer samost vahama a druhy samost vahama

    # Uncomment to get a better accuracy - it doesn't really correspond to RCGN
    template += (R.h_2(V.I) <= (R.h_1(V.J)[W2: w, w], R.hidden.edge(V.J, relation, V.I))) | metadata

template += R.h_2 / 1 | [Activation.SIGMOID]

template += R.predict(V.X, V.Y)[20, w] <= (R.h_2(V.X)[w, w], R.h_2(V.Y)[w, w])
template += R.predict / 2 | [Activation.SIGMOID]


#%% custom, using relation parametrization

def weight_name(rel: str, weight: str):
    return f'{weight}_{rel.replace("-", "_")}'

metadata = [Aggregation.AVG, Activation.IDENTITY]

template = Template()

w = 24

template += R.person_1(V.X)[w, ] <= R.person(V.X)  # Project into vector

# create relation embeddings
template += [(R.rel_embed(r)[w, ] <= R.rel(r)) for r in relation_lst]

for relation in relation_lst:
    W1 = weight_name(relation, "W1")
    template += (
        R.h_1(relation) <= (R.person_1(V.I), R.person_2(V.J), R.rel_embed(relation)[W1: w, w], R.hidden.edge(V.I, relation, V.J))
    ) | metadata

template += R.h_1 / 1 | [Activation.SIGMOID]

template += R.predict(V.X, V.Y)[20, w] <= (R.h_1(V.R)[w, w], R.hidden.edge(V.X, V.R, V.W), R.hidden.edge(V.Y, V.R, V.Z))
template += R.predict / 2 | [Activation.SIGMOID]

#%%

import sys
from neuralogic.logging import add_handlers, clear_handlers, LEVEL

clear_handlers()
# add_handlers(sys.stdout, level=LEVEL.WARNING)
#

#%%
"""## Training"""


from neuralogic.core import Backend, Settings, Optimizer, ErrorFunction
from neuralogic.nn import get_evaluator

n_epochs = 24

settings = Settings(optimizer=Optimizer.ADAM, epochs=n_epochs, learning_rate=0.0005,
                    error_function=ErrorFunction.CROSSENTROPY)
evaluator = get_evaluator(template, Backend.JAVA, settings)


#%%

import matplotlib.pyplot as plt


average_losses = []

for i, (current_total_loss, number_of_samples) in enumerate(evaluator.train(dataset)):
    average_loss = current_total_loss / number_of_samples
    print(f'Epoch: {i:03d}, Train Loss: {average_loss}')
    average_losses.append(current_total_loss / number_of_samples)

    if average_loss < 1e-3:
        print("Early stopping")
        break


#%%

print(average_losses)
print(dataset)
#%%

import numpy as np
from collections import Counter


def report_distribution(labels):
    labels_c = Counter(labels)
    print("[" + ", ".join(f"{i}: {labels_c[i]:2d}" for i in range(len(relation_to_idx))) + "]")


def test(dataset: Dataset) -> float:
    predictions = []
    labels = []

    correct = 0
    for y, y_hat in evaluator.test(dataset, generator=False):
        pred = np.argmax(y_hat)
        true_label = np.argmax(y)
        predictions.append(pred)
        labels.append(true_label)
        if pred == true_label:
            correct += 1

    report_distribution(labels)
    report_distribution(predictions)

    return float(correct) / len(dataset)


#%%

train_accuracy = test(dataset)
print(f'Train set Accuracy: {train_accuracy:.7f}')

#%%

reports = []
for test_name, test_dataset in zip(test_names, test_datasets):
    test_accuracy = test(test_dataset)
    report = f'Test Set: {test_name.rsplit("/")[-1]}, Samples: {len(test_dataset)}, Accuracy: {test_accuracy:.7f}'
    reports.append(report)
    print(report)

for report in reports:
    print(report)

#%%
print(relation_to_idx)

